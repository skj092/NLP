{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing the libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q torchtext ","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:00:48.653173Z","iopub.execute_input":"2022-03-30T04:00:48.653810Z","iopub.status.idle":"2022-03-30T04:00:58.177948Z","shell.execute_reply.started":"2022-03-30T04:00:48.653668Z","shell.execute_reply":"2022-03-30T04:00:58.177005Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchtext\nimport torch.nn as nn\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-30T04:00:58.180406Z","iopub.execute_input":"2022-03-30T04:00:58.180791Z","iopub.status.idle":"2022-03-30T04:00:59.583013Z","shell.execute_reply.started":"2022-03-30T04:00:58.180743Z","shell.execute_reply":"2022-03-30T04:00:59.582083Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/nlp-getting-started/train.csv') \ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:00:59.586742Z","iopub.execute_input":"2022-03-30T04:00:59.587003Z","iopub.status.idle":"2022-03-30T04:00:59.659883Z","shell.execute_reply.started":"2022-03-30T04:00:59.586972Z","shell.execute_reply":"2022-03-30T04:00:59.658922Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# shuffling the dataset\ndf = df.sample(frac=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:00:59.662264Z","iopub.execute_input":"2022-03-30T04:00:59.662621Z","iopub.status.idle":"2022-03-30T04:00:59.672462Z","shell.execute_reply.started":"2022-03-30T04:00:59.662566Z","shell.execute_reply":"2022-03-30T04:00:59.671681Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:00:59.673800Z","iopub.execute_input":"2022-03-30T04:00:59.674936Z","iopub.status.idle":"2022-03-30T04:00:59.685264Z","shell.execute_reply.started":"2022-03-30T04:00:59.674885Z","shell.execute_reply":"2022-03-30T04:00:59.684146Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(7613, 5)"},"metadata":{}}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:00:59.686866Z","iopub.execute_input":"2022-03-30T04:00:59.687197Z","iopub.status.idle":"2022-03-30T04:00:59.708541Z","shell.execute_reply.started":"2022-03-30T04:00:59.687154Z","shell.execute_reply":"2022-03-30T04:00:59.707670Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"         id              keyword               location  \\\n2569   3684              destroy                    NaN   \n4149   5898                 harm            Kansas City   \n767    1110            blew%20up  california mermaid ?    \n7482  10704                wreck       Atlanta, Georgia   \n1228   1769  buildings%20burning       Washington, D.C.   \n\n                                                   text  target  \n2569  (SJ GIST): 148 Houses Farm Produce Destroy... ...       1  \n4149  @dinallyhot Love what you picked! We're playin...       0  \n767   Some guy whistled at me in the parking lot &am...       0  \n7482  #Trump debate will be most highly watched show...       0  \n1228  Watching Xela firefighters struggle to save bu...       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2569</th>\n      <td>3684</td>\n      <td>destroy</td>\n      <td>NaN</td>\n      <td>(SJ GIST): 148 Houses Farm Produce Destroy... ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4149</th>\n      <td>5898</td>\n      <td>harm</td>\n      <td>Kansas City</td>\n      <td>@dinallyhot Love what you picked! We're playin...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>767</th>\n      <td>1110</td>\n      <td>blew%20up</td>\n      <td>california mermaid ?</td>\n      <td>Some guy whistled at me in the parking lot &amp;am...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7482</th>\n      <td>10704</td>\n      <td>wreck</td>\n      <td>Atlanta, Georgia</td>\n      <td>#Trump debate will be most highly watched show...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1228</th>\n      <td>1769</td>\n      <td>buildings%20burning</td>\n      <td>Washington, D.C.</td>\n      <td>Watching Xela firefighters struggle to save bu...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# training and validation set\n\ntrain_df = df[:6090]\nvalid_df = df[6090:6850]\ntest_df = df[6850:]\n\ntrain_df.shape, valid_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:05:53.240269Z","iopub.execute_input":"2022-03-30T04:05:53.240661Z","iopub.status.idle":"2022-03-30T04:05:53.249333Z","shell.execute_reply.started":"2022-03-30T04:05:53.240623Z","shell.execute_reply":"2022-03-30T04:05:53.248557Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"((6090, 5), (760, 5), (763, 5))"},"metadata":{}}]},{"cell_type":"markdown","source":"**Dataset**","metadata":{}},{"cell_type":"code","source":"class Data(Dataset):\n    def __init__(self, df):\n        self.df = df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        text = self.df.iloc[idx]['text']\n        label = self.df.iloc[idx]['target']\n        return text, label","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:05:54.261173Z","iopub.execute_input":"2022-03-30T04:05:54.261460Z","iopub.status.idle":"2022-03-30T04:05:54.266678Z","shell.execute_reply.started":"2022-03-30T04:05:54.261429Z","shell.execute_reply":"2022-03-30T04:05:54.265950Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_dataset = Data(train_df)\nvalid_dataset = Data(valid_df)\ntest_dataset = Data(test_df)\nlen(train_dataset), len(valid_dataset), len(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:06:09.799734Z","iopub.execute_input":"2022-03-30T04:06:09.799986Z","iopub.status.idle":"2022-03-30T04:06:09.805915Z","shell.execute_reply.started":"2022-03-30T04:06:09.799956Z","shell.execute_reply":"2022-03-30T04:06:09.805050Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"(6090, 760, 763)"},"metadata":{}}]},{"cell_type":"code","source":"ds = Data(df)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:06:12.049871Z","iopub.execute_input":"2022-03-30T04:06:12.050179Z","iopub.status.idle":"2022-03-30T04:06:12.054304Z","shell.execute_reply.started":"2022-03-30T04:06:12.050145Z","shell.execute_reply":"2022-03-30T04:06:12.053595Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"**tokenization**","metadata":{}},{"cell_type":"code","source":"# finding unique tokens (words)\n\nimport re\nfrom collections import Counter, OrderedDict\nfrom nltk.tokenize import word_tokenize\n\ndef tokenizer(text):\n    tokenized = word_tokenize(text)\n    return tokenized\n\ntoken_counts = Counter()\n\nfor text, label in ds:\n    tokens = tokenizer(text)\n    token_counts.update(tokens)\n\nprint('Vocab length', len(token_counts))","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:06:14.331184Z","iopub.execute_input":"2022-03-30T04:06:14.331450Z","iopub.status.idle":"2022-03-30T04:06:18.143914Z","shell.execute_reply.started":"2022-03-30T04:06:14.331422Z","shell.execute_reply":"2022-03-30T04:06:18.143042Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Vocab length 27291\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**numericalization**","metadata":{}},{"cell_type":"code","source":"## Step 3: encoding each unique token into integers\nfrom torchtext.vocab import vocab\n\nsorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\nordered_dict = OrderedDict(sorted_by_freq_tuples)\n\nvocab = vocab(ordered_dict)\n\nvocab.insert_token(\"<pad>\", 0)\nvocab.insert_token(\"<unk>\", 1)\nvocab.set_default_index(1)\n\nprint([vocab[token] for token in ['this', 'is', 'an', 'example']])","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:06:18.145693Z","iopub.execute_input":"2022-03-30T04:06:18.146174Z","iopub.status.idle":"2022-03-30T04:06:18.280013Z","shell.execute_reply.started":"2022-03-30T04:06:18.146116Z","shell.execute_reply":"2022-03-30T04:06:18.279128Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"[42, 17, 60, 3299]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Dataloader**","metadata":{}},{"cell_type":"code","source":"# Step 3-A: define the functions for transformation\n\n# device = torch.device(\"cuda:0\")\ndevice = 'cpu'\n\ntext_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\nlabel_pipeline = lambda x: 1. if x == 'pos' else 0.\n\n\n## Step 3-B: wrap the encode and transformation function\ndef collate_batch(batch):\n    label_list, text_list, lengths = [], [], []\n    for _text, _label in batch:\n        label_list.append(label_pipeline(_label))\n        processed_text = torch.tensor(text_pipeline(_text), \n                                      dtype=torch.int64)\n        text_list.append(processed_text)\n        lengths.append(processed_text.size(0))\n    label_list = torch.tensor(label_list)\n    lengths = torch.tensor(lengths)\n    padded_text_list = nn.utils.rnn.pad_sequence(\n        text_list, batch_first=True)\n    return padded_text_list.to(device), label_list.to(device), lengths.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:06:21.534223Z","iopub.execute_input":"2022-03-30T04:06:21.534516Z","iopub.status.idle":"2022-03-30T04:06:21.542874Z","shell.execute_reply.started":"2022-03-30T04:06:21.534483Z","shell.execute_reply":"2022-03-30T04:06:21.541835Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"## Take a small batch\n\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\ntext_batch, label_batch, length_batch = next(iter(dataloader))\nprint(text_batch)\nprint(label_batch)\nprint(length_batch)\nprint(text_batch.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:06:27.174513Z","iopub.execute_input":"2022-03-30T04:06:27.175002Z","iopub.status.idle":"2022-03-30T04:06:27.186911Z","shell.execute_reply.started":"2022-03-30T04:06:27.174969Z","shell.execute_reply":"2022-03-30T04:06:27.186168Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"tensor([[  41, 8169, 8170,   38,    2, 8171, 2036, 5285, 8172, 1821,   16,    3,\n            2, 8173,    4, 8174,   83,   34,    2, 8175,    0,    0,    0,    0,\n            0,    0,    0,    0,    0],\n        [   7, 8176,  517,   87,   22, 2037,   15,  130,   91, 1054, 3170,  985,\n           31, 3982, 3983, 3171,  172,   12,   22,   15, 1314,   32,   43,   25,\n         2644,    2,    3,    2, 3984],\n        [ 330,  752, 8177,   30,   46,   11,    8, 3172,  414,   32,   43,   25,\n           24,  138,   53,  233,   27,    8,  415,  491,   28, 3985,   48,  249,\n           11,    8,  154,    5,    5],\n        [   4, 2038, 2297,   58,   36,  295, 3986, 1822,  545,  387,  194, 1823,\n         8178,    6,   13,  168,   58,  342, 3987,   18,    9, 5286, 3988,  260,\n            6,    0,    0,    0,    0]])\ntensor([0., 0., 0., 0.])\ntensor([20, 29, 29, 25])\ntorch.Size([4, 29])\n","output_type":"stream"}]},{"cell_type":"code","source":"## Step 4: batching the datasets\n\nbatch_size = 32  \n\ntrain_dl = DataLoader(train_dataset, batch_size=batch_size,\n                      shuffle=True, collate_fn=collate_batch)\nvalid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n                      shuffle=False, collate_fn=collate_batch)\ntest_dl = DataLoader(test_dataset, batch_size=batch_size,\n                     shuffle=False, collate_fn=collate_batch)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:06:36.270424Z","iopub.execute_input":"2022-03-30T04:06:36.270717Z","iopub.status.idle":"2022-03-30T04:06:36.276360Z","shell.execute_reply.started":"2022-03-30T04:06:36.270688Z","shell.execute_reply":"2022-03-30T04:06:36.275205Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"len(train_dl), len(valid_dl), len(test_dl)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:06:41.102689Z","iopub.execute_input":"2022-03-30T04:06:41.103501Z","iopub.status.idle":"2022-03-30T04:06:41.109655Z","shell.execute_reply.started":"2022-03-30T04:06:41.103426Z","shell.execute_reply":"2022-03-30T04:06:41.108533Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"(191, 24, 24)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"code","source":"embedding = nn.Embedding(num_embeddings=10, \n                         embedding_dim=3, \n                         padding_idx=0)\n \n# a batch of 2 samples of 4 indices each\ntext_encoded_input = torch.LongTensor([[1,2,4,5],[4,3,2,0]])\nprint(embedding(text_encoded_input))","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:06:43.638958Z","iopub.execute_input":"2022-03-30T04:06:43.639608Z","iopub.status.idle":"2022-03-30T04:06:43.646229Z","shell.execute_reply.started":"2022-03-30T04:06:43.639574Z","shell.execute_reply":"2022-03-30T04:06:43.645435Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"tensor([[[-1.0233, -0.5962, -1.0055],\n         [-0.2106, -0.0075, -1.7869],\n         [-0.9962, -0.8313,  1.3075],\n         [-1.1628,  0.1196, -0.1631]],\n\n        [[-0.9962, -0.8313,  1.3075],\n         [ 1.6103, -0.7040, -0.1853],\n         [-0.2106, -0.0075, -1.7869],\n         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>)\n","output_type":"stream"}]},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, \n                                      embed_dim, \n                                      padding_idx=0) \n        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n                           batch_first=True)\n        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(fc_hidden_size, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, text, lengths):\n        out = self.embedding(text)\n        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n        out, (hidden, cell) = self.rnn(out)\n        out = hidden[-1, :, :]\n        out = self.fc1(out)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.sigmoid(out)\n        return out\n         \nvocab_size = len(vocab)\nembed_dim = 20\nrnn_hidden_size = 64\nfc_hidden_size = 64\n\ntorch.manual_seed(1)\nmodel = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:06:45.890606Z","iopub.execute_input":"2022-03-30T04:06:45.890891Z","iopub.status.idle":"2022-03-30T04:06:45.908459Z","shell.execute_reply.started":"2022-03-30T04:06:45.890861Z","shell.execute_reply":"2022-03-30T04:06:45.907746Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def train(dataloader):\n    model.train()\n    total_acc, total_loss = 0, 0\n    for text_batch, label_batch, lengths in dataloader:\n        optimizer.zero_grad()\n        pred = model(text_batch, lengths)[:, 0]\n        loss = loss_fn(pred, label_batch)\n        loss.backward()\n        optimizer.step()\n        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n        total_loss += loss.item()*label_batch.size(0)\n    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n \ndef evaluate(dataloader):\n    model.eval()\n    total_acc, total_loss = 0, 0\n    with torch.no_grad():\n        for text_batch, label_batch, lengths in dataloader:\n            pred = model(text_batch, lengths)[:, 0]\n            loss = loss_fn(pred, label_batch)\n            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n            total_loss += loss.item()*label_batch.size(0)\n    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:06:48.601462Z","iopub.execute_input":"2022-03-30T04:06:48.602045Z","iopub.status.idle":"2022-03-30T04:06:48.610757Z","shell.execute_reply.started":"2022-03-30T04:06:48.602009Z","shell.execute_reply":"2022-03-30T04:06:48.609703Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 10 \n\ntorch.manual_seed(1)\n \nfor epoch in range(num_epochs):\n    acc_train, loss_train = train(train_dl)\n    acc_valid, loss_valid = evaluate(valid_dl)\n    print(f'Epoch {epoch} train_loss: {loss_train:.4f} valid_loss: {loss_valid:.4f} val_accuracy: {acc_valid:.4f}')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:08:42.295166Z","iopub.execute_input":"2022-03-30T04:08:42.295718Z","iopub.status.idle":"2022-03-30T04:09:50.177067Z","shell.execute_reply.started":"2022-03-30T04:08:42.295678Z","shell.execute_reply":"2022-03-30T04:09:50.176128Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Epoch 0 train_loss: 0.0864 valid_loss: 0.0020 val_accuracy: 1.0000\nEpoch 1 train_loss: 0.0008 valid_loss: 0.0007 val_accuracy: 1.0000\nEpoch 2 train_loss: 0.0004 valid_loss: 0.0003 val_accuracy: 1.0000\nEpoch 3 train_loss: 0.0002 valid_loss: 0.0002 val_accuracy: 1.0000\nEpoch 4 train_loss: 0.0001 valid_loss: 0.0001 val_accuracy: 1.0000\nEpoch 5 train_loss: 0.0001 valid_loss: 0.0001 val_accuracy: 1.0000\nEpoch 6 train_loss: 0.0000 valid_loss: 0.0000 val_accuracy: 1.0000\nEpoch 7 train_loss: 0.0000 valid_loss: 0.0000 val_accuracy: 1.0000\nEpoch 8 train_loss: 0.0000 valid_loss: 0.0000 val_accuracy: 1.0000\nEpoch 9 train_loss: 0.0000 valid_loss: 0.0000 val_accuracy: 1.0000\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}